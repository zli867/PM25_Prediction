{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5846c911cb92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Reformat the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../data/220720PM25diffsite.csv\"\n",
    "output_file = \"../result/nn_selected_features.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\"pm25\", \"ENSOmonthly\"\n",
    "           ,\"eNOx\",\"SO2emis\",\"PM25emis\",\"eVOC\",\"NH3emis\"\n",
    "           ,\"TMAXbarstow\",\"AWNDLAX\",\"Mir850RH\",\"Rhontario\"\n",
    "           ,\"dayofweekf\",\"dayofyear\"]\n",
    "all_features = [\"pm25\", \"ENSOmonthly\"\n",
    "              ,\"eNOx\",\"SO2emis\",\"PM25emis\",\"eVOC\",\"NH3emis\"\n",
    "              ,\"TMAXbarstow\",\"AWNDLAX\",\"Mir850RH\",\"Rhontario\"\n",
    "              ,\"dayofweekf\",\"dayofyear\"\n",
    "              ,\"MirTemp500C\",\"MirWS850ms\",\"MirWD850\",\"MirHeight850\",\"MirWS500ms\",\"MirWD500\",\"Mir500RH\"\n",
    "              ,\"SRmeanC\",\"AWNDbarstow\",\"TMAXLAX\",\"TMAXontario\",\"AWNDontario\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_features = df[all_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide the features (for all features or selected features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df_all_features.dropna()\n",
    "label_name = \"pm25\"\n",
    "y_vector = dataset[[label_name]]\n",
    "# change it for all features or selected features\n",
    "# features_names = all_features.copy()\n",
    "features_names = selected_features.copy()\n",
    "features_names.remove(label_name)\n",
    "X_matrix = dataset[features_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dayofweekToNum(data_frame):\n",
    "    day_mapping = {\"Mon\": 1, \"Tue\": 2, \"Wed\": 3, \"Thu\": 4, \"Fri\": 5, \"Sat\": 6, \"Sun\": 7}\n",
    "    dayofweekf = data_frame[\"dayofweekf\"].to_numpy()\n",
    "    res = []\n",
    "    for i in range(0, len(dayofweekf)):\n",
    "        res.append(day_mapping[dayofweekf[i]])\n",
    "    data_frame.loc[:, (\"dayofweekf\")] = res\n",
    "    return data_frame\n",
    "print(X_matrix)\n",
    "X_matrix = dayofweekToNum(X_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = X_matrix.to_numpy()\n",
    "_, num_features = features_data.shape\n",
    "label_data = y_vector.to_numpy()\n",
    "# split the data for 10-fold cross validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE ACTIVATION FUNCTION\n",
    "# ACTIVATION = torch.tanh\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_features, num_layers, num_hiddens, ACTIVATION):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.fcs = []\n",
    "        self.activate = ACTIVATION\n",
    "        # Define input layer\n",
    "        self.input_fc = nn.Linear(num_features, num_hiddens)\n",
    "        # Define hidden layers\n",
    "        for i in range(0, num_layers):\n",
    "            fc = nn.Linear(num_hiddens, num_hiddens)\n",
    "            setattr(self, 'fc%i' % i, fc)\n",
    "            self.fcs.append(fc)\n",
    "        # Define output layers\n",
    "        self.output_layer = nn.Linear(num_hiddens, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_fc(x)\n",
    "        for i in range(0, self.num_layers):\n",
    "            x = self.fcs[i](x)\n",
    "            x = self.activate(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss(reduction ='mean')\n",
    "def loss_func(model, features, true_values):\n",
    "    predict_values = model.forward(features)\n",
    "    res = loss_function(predict_values, true_values)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "ReLU activation function, 3 hidden layers, 40 neuron for each hidden layers; 5000 training batch; Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters and net\n",
    "ACTIVATION = torch.relu\n",
    "# select feature 3 layers and 40 nodes; all feature 2 layer and 60 nodes\n",
    "net_parameters = [(3, 40)]\n",
    "max_iter = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = torch.from_numpy(features_data).float()\n",
    "label_data = torch.from_numpy(label_data).float()\n",
    "data_rows = []\n",
    "training_data_rows = []\n",
    "final_r2 = 0\n",
    "final_rmse = 0\n",
    "final_mbe = 0\n",
    "final_prediction = None\n",
    "for net_parameter in net_parameters:\n",
    "    r2_res = []\n",
    "    rmse_res = []\n",
    "    r2_res_training = []\n",
    "    rmse_res_training = []\n",
    "    for train_index, test_index in kf.split(features_data):\n",
    "        # build the net\n",
    "        net = NeuralNetwork(num_features, net_parameter[0], net_parameter[1], ACTIVATION)\n",
    "        optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "        X_train, X_test = features_data[train_index], features_data[test_index]\n",
    "        y_train, y_test = label_data[train_index], label_data[test_index]\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            loss = loss_func(net, X_train, y_train)\n",
    "            optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "            loss.backward() #backprop\n",
    "            optimizer.step()\n",
    "        print(loss)\n",
    "        # evaluate the model r2 using test data set, etc.\n",
    "        net.eval()\n",
    "        # test data performance\n",
    "        predict_res = net(X_test)\n",
    "        predict_res = predict_res.detach().numpy()\n",
    "        r2_res.append(r2_score(y_test, predict_res))\n",
    "        rmse_res.append(mean_squared_error(predict_res, y_test, squared=False))\n",
    "        # training data performance\n",
    "        predict_res = net(X_train)\n",
    "        predict_res = predict_res.detach().numpy()\n",
    "        r2_res_training.append(r2_score(y_train, predict_res))\n",
    "        rmse_res_training.append(mean_squared_error(predict_res, y_train, squared=False))\n",
    "    # evaluate the model, r2, rmse\n",
    "    # for testing part\n",
    "    r2_mean = np.mean(r2_res)\n",
    "    rmse_mean = np.mean(rmse_res)\n",
    "    data_row = [net_parameter[0], net_parameter[1], r2_mean, rmse_mean]\n",
    "    data_rows.append(data_row)\n",
    "    # for training part\n",
    "    r2_mean = np.mean(r2_res_training)\n",
    "    rmse_mean = np.mean(rmse_res_training)\n",
    "    data_row = [net_parameter[0], net_parameter[1], r2_mean, rmse_mean]\n",
    "    training_data_rows.append(data_row)\n",
    "    # train by all data\n",
    "    net = NeuralNetwork(num_features, net_parameter[0], net_parameter[1], ACTIVATION)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "    for i in range(max_iter):\n",
    "        loss = loss_func(net, features_data, label_data)\n",
    "        optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "        loss.backward() #backprop\n",
    "        optimizer.step()\n",
    "        # evaluate the model r2 using test data set, etc.\n",
    "    net.eval()\n",
    "    predict_res = net(features_data)\n",
    "    predict_res = predict_res.detach().numpy()\n",
    "    final_r2 = r2_score(label_data, predict_res)\n",
    "    final_rmse = mean_squared_error(predict_res, label_data, squared=False)\n",
    "    final_mbe = np.mean(predict_res - label_data.numpy())\n",
    "    final_prediction = predict_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork(num_features, net_parameters[0][0], net_parameters[0][1], ACTIVATION)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Table S4\n",
    "print(\"Training Data\")\n",
    "print(\"R2 = %f   RMSE = %f\" %(training_data_rows[0][2], training_data_rows[0][3]))\n",
    "print(\"Testing Data\")\n",
    "print(\"R2 = %f   RMSE = %f\" %(data_rows[0][2], data_rows[0][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Table 1\n",
    "print(\"R2 = %f   RMSE = %f    MBE = %f\" %(final_r2, final_rmse, final_mbe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data for Annual Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_matrix['predict_values'] = final_prediction\n",
    "# save to csv\n",
    "X_matrix.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
